C'est une excellente question. Pour un profil ingénieur/entrepreneur comme le tien, comprendre ce qu'il y a "sous le capot" est essentiel. Contrairement à une application classique (type Airbnb ou Facebook) qui a un "vrai" backend (un serveur Python ou Node.js connecté à une base de données MySQL), ton application Event Clock AI repose sur une architecture moderne appelée "Serverless Client-Side AI". En gros : Le cerveau, c'est Gemini. Le corps, c'est ton navigateur. Voici le détail étape par étape de ce qui se passe techniquement entre le moment où tu cliques sur "Lancer le Scan" et l'affichage des résultats.

L'Initialisation (Le Briefing) Quand tu cliques sur le bouton, ton navigateur (le client React) prépare un paquet de données à envoyer aux serveurs de Google. Ce paquet contient deux choses essentielles : L'Identité (API Key) : Ton passe-port qui t'autorise à utiliser les serveurs de Google. Le Prompt Système (La consigne) : C'est un long texte caché dans le code (index.tsx) qui dit littéralement à l'IA : "Tu es un chasseur d'événements. Cherche en Île-de-France. Cherche des sujets Tech, Ingénierie, Business. Et surtout, renvoie-moi les données en JSON strict."
Le "Grounding" (La magie de la recherche) C'est ici que la différence se fait. Si on utilisait juste ChatGPT ou Gemini "nu", ils inventeraient des événements ou te donneraient des vieux trucs de 2023. Pour éviter ça, on utilise le paramètre { tools: [{ googleSearch: {} }] }. Voici la mécanique interne chez Google : Analyse de l'intention : Le modèle reçoit ton prompt ("Trouve des événements tech en IDF"). Génération de requêtes (Querying) : Le modèle comprend qu'il ne sait pas la réponse. Il se transforme en robot de recherche. Il va générer tout seul des requêtes Google en temps réel, par exemple : "Hackathon Paris novembre 2025" "Salon étudiant ingénieur Île-de-France 2025" "Conférence Station F agenda" Lecture des résultats (Retrieval) : Google Search lui renvoie les 10-20 premiers résultats de ces recherches (titres, petits résumés, dates, liens). Synthèse (Reasoning) : Le modèle lit ces bouts de texte (snippets). Il agit comme un humain qui lirait 10 pages web en 2 secondes. Il extrait les infos : "Ah, là je vois une date", "Là c'est le prix", "Là c'est l'adresse".
La Structuration (Le Parsing) Le web est sale. C'est du HTML, des pubs, du texte dans tous les sens. Ton application, elle, veut un tableau propre. C'est là que la puissance du LLM (Large Language Model) intervient. On lui a ordonné de répondre en JSON. Il va prendre les données brutes qu'il a trouvées via la recherche et les forcer à rentrer dans tes cases : Il met le texte "24 oct" dans la case date. Il met "Station F" dans la case location. Il invente une courte description basée sur ce qu'il a lu. Le backend de Google renvoie alors un gros bloc de texte qui ressemble à ça : code JSON [ { "title": "AI Night", "date": "Jeudi...", "location": "Paris..." }, { "title": "Salon des Grandes Écoles", ... } ]
La Réception et le Stockage (Le "Faux" Backend) Ton navigateur reçoit ce texte. Nettoyage : Le code Javascript retire les balises Markdown éventuelles (les ```json). Parsing : Il transforme le texte en objets Javascript manipulables. Affichage : React dessine les cartes à l'écran. Persistance (Ton "Database") : C'est là que ton historique intervient. Puisqu'on n'a pas de serveur de base de données, on utilise le Local Storage du navigateur. Le code prend ce tableau d'événements. Il le transforme en chaîne de caractères. Il l'écrit sur le disque dur de ton ordinateur, dans un petit fichier géré par Chrome/Edge. C'est pour ça que quand tu reviens demain, tes données sont encore là (tant que tu ne vides pas le cache).
La Fusion (Le Master View) Quand tu vas dans l'onglet "Répertoire", le code fait ceci : Il ouvre tous les petits fichiers d'historique stockés. Il les met tous dans un grand sac. Il lance l'algo de Déduplication que j'ai écrit : il compare les signatures (titre + date). Si "Event A" du scan d'hier est le même que "Event A" du scan d'aujourd'hui, il les fusionne pour n'en garder qu'un. En résumé Tu as créé un "Moteur de Recherche Structuré". Avant : Tu devais aller sur Google, taper 10 recherches, ouvrir 20 onglets, lire, copier-coller dans Excel. Maintenant : Tu envoies l'agent IA faire exactement ces actions à ta place, à la vitesse de la lumière, et il te rend le fichier Excel fini. C'est ça, la puissance de l'IA appliquée à l'ingénierie de données ("Data Engineering").
